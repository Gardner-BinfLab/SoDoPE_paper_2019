{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "\n",
    "def fasta_reader(file):\n",
    "    '''Converts .fasta to a pandas dataframe with accession as index\n",
    "    and sequence in a column 'sequence'\n",
    "    '''\n",
    "    fasta_df = pd.read_csv(file, sep='>', lineterminator='>', header=None)\n",
    "    fasta_df[['Accession', 'Sequence']] = fasta_df[0].str.split('\\n', 1, \\\n",
    "                                        expand=True)\n",
    "    fasta_df['Accession'] = fasta_df['Accession']\n",
    "    fasta_df['Sequence'] = fasta_df['Sequence'].replace('\\n', '', regex=True).\\\n",
    "                            astype(str).str.upper().replace('U', 'T')\n",
    "    total_seq = fasta_df.shape[0]\n",
    "    fasta_df.drop(0, axis=1, inplace=True)\n",
    "    fasta_df = fasta_df[fasta_df.Sequence != '']\n",
    "    fasta_df = fasta_df[fasta_df.Sequence != 'NONE']\n",
    "    final_df = fasta_df.dropna()\n",
    "    remained_seq = final_df.shape[0]\n",
    "    if total_seq != remained_seq:\n",
    "        print(\"{} sequences were removed due to inconsistencies in\"\n",
    "                      \"provided file.\".format(total_seq-remained_seq))\n",
    "    return final_df\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4368"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = root_dir + '/Cross_validation/Clustering/10'\n",
    "clusters = []\n",
    "\n",
    "\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    try:\n",
    "        clusters.append(fasta_reader(os.path.join(path, file)))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "len(clusters) #Number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n",
      "Pass\n",
      "Pass\n",
      "Pass\n",
      "Pass\n",
      "Pass\n",
      "Pass\n",
      "Pass\n",
      "Pass\n",
      "Last item reached.\n"
     ]
    }
   ],
   "source": [
    "#remove character ----- from alignment\n",
    "\n",
    "for index, c in enumerate(clusters):\n",
    "    c['Prot'] = c['Sequence'].str.replace('-','')\n",
    "    c['Cluster'] = index\n",
    "\n",
    "all_clusters = pd.concat(clusters)\n",
    "all_clusters = all_clusters.reset_index(drop=True)\n",
    "\n",
    "#we will merge this with full pET sequences based on accession, so we dont need these\n",
    "#protein sequecnes from here because tags were removed before clustering\n",
    "#After merging with pET sequences based on accession, we will get tag added sequences\n",
    "\n",
    "df_ = all_clusters[['Accession', 'Cluster']].copy()\n",
    "\n",
    "#We want 10 sets each covering almost 10% of sequences (~1221 sequences per cluster)\n",
    "df0 = df_.loc[0:1224]\n",
    "df1 = df_.loc[1225:2440]\n",
    "df2 = df_.loc[2441:3673]\n",
    "df3 = df_.loc[3674:4915]\n",
    "df4 = df_.loc[4916:6116]\n",
    "df5 = df_.loc[6117:7335]\n",
    "df6 = df_.loc[7336:8555]\n",
    "df7 = df_.loc[8556:9787]\n",
    "df8 = df_.loc[9788:10997]\n",
    "df9 = df_.loc[10998:12216]\n",
    "\n",
    "dfs_cv = [df0, df1, df2, df3, df4, df5, df6, df7, df8, df9]\n",
    "#check for seperation between clusters in these dfs\n",
    "\n",
    "for i, d in enumerate(dfs_cv):\n",
    "    try:\n",
    "        #check if tail of one overlaps head of next cluster\n",
    "        if d.tail(1)['Cluster'].values != dfs_cv[i+1].head(1)['Cluster'].values:\n",
    "            print('Pass')\n",
    "        else:\n",
    "            print('Overlapped dfs! : ', i, ' and ', i + 1)\n",
    "    except Exception as exp: #catch for off by one error\n",
    "        print('Last item reached.')\n",
    "\n",
    "cv_test = [] #one df per item in list\n",
    "cv_train = [] #nine remiaining df per item in list\n",
    "for i, v in enumerate(dfs_cv):\n",
    "    cv_test.append(v) #pick for testing\n",
    "    cv_train.append([x for j, x in enumerate(dfs_cv) if j!=i ]) #pick remaining for training\n",
    "\n",
    "\n",
    "    \n",
    "cv_train_concat_nine = [pd.concat(i) for i in cv_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pET = pd.read_pickle(root_dir + '/results/pET_complete.pkl.gz')\n",
    "\n",
    "#merge with cleaned pET to remove problematic sequences\n",
    "\n",
    "\n",
    "training_merged = [i.merge(pET, on='Accession') for i in cv_train_concat_nine]\n",
    "testing_merged = [i.merge(pET, on='Accession') for i in cv_test]\n",
    "\n",
    "\n",
    "with open(root_dir + '/results/training_10_with_tag.pkl', 'wb') as handle:\n",
    "    pickle.dump(training_merged, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(root_dir + '/results/testing_10_with_tag.pkl', 'wb') as handle:\n",
    "    pickle.dump(testing_merged, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export tags without tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/userdata/student_users/bikashkumarbhandari/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Volumes/userdata/student_users/bikashkumarbhandari/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "pET_notag = pET.copy()\n",
    "pET_notag['Protein']['pET15'] = pET_notag['Protein']['pET15'].apply(lambda x:x[10:])\n",
    "pET_notag['Protein']['pET21'] = pET_notag['Protein']['pET21'].apply(lambda x:x[:-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_merged_notag = [i.merge(pET_notag, on='Accession') for i in cv_train_concat_nine]\n",
    "testing_merged_notag = [i.merge(pET_notag, on='Accession') for i in cv_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root_dir + '/results/training_10_without_tag.pkl', 'wb') as handle:\n",
    "    pickle.dump(training_merged_notag, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(root_dir + '/results/testing_10_without_tag.pkl', 'wb') as handle:\n",
    "    pickle.dump(testing_merged_notag, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
